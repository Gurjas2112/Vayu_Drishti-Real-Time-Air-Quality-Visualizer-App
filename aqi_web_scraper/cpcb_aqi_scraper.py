"""
CPCB Air Quality Data Scraper

This script scrapes real-time and historical air quality data from the 
Central Pollution Control Board (CPCB) official dashboard.

Author: Generated by GitHub Copilot
Date: September 8, 2025
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import logging
from datetime import datetime
import re
from typing import List, Dict, Optional
import urllib.parse

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cpcb_scraper.log'),
        logging.StreamHandler()
    ]
)

class CPCBAirQualityScraper:
    """
    A comprehensive scraper for CPCB air quality data
    """
    
    def __init__(self):
        """Initialize the scraper with session and headers"""
        self.session = requests.Session()
        
        # Set user agent and headers to mimic a real browser
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session.headers.update(self.headers)
        
        # Base URLs
        self.base_url = "https://app.cpcbccr.com/AQI_India/"
        self.api_base_url = "https://app.cpcbccr.com/ccr_docs/"
        
        # Pollutant mapping
        self.pollutants = ['PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3', 'NH3', 'Pb']
        
        # Rate limiting
        self.request_delay = 2  # seconds between requests
        
        # Data storage
        self.all_data = []
        
    def make_request(self, url: str, params: Optional[Dict] = None, timeout: int = 30) -> Optional[requests.Response]:
        """
        Make a GET request with error handling and rate limiting
        
        Args:
            url: URL to request
            params: Query parameters
            timeout: Request timeout in seconds
            
        Returns:
            Response object or None if failed
        """
        try:
            # Add delay to respect rate limits
            time.sleep(self.request_delay)
            
            response = self.session.get(url, params=params, timeout=timeout)
            response.raise_for_status()
            
            logging.info(f"Successfully fetched: {url}")
            return response
            
        except requests.exceptions.RequestException as e:
            logging.error(f"Request failed for {url}: {str(e)}")
            return None
    
    def extract_state_city_data(self) -> List[Dict]:
        """
        Extract available states and cities from the main page
        
        Returns:
            List of dictionaries containing state and city information
        """
        logging.info("Extracting state and city data...")
        
        response = self.make_request(self.base_url)
        if not response:
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Look for state/city selection elements
        states_cities = []
        
        # Try to find dropdown or selection elements
        state_selects = soup.find_all(['select', 'option'], {'name': re.compile(r'state|city', re.I)})
        
        # Also look for JavaScript data that might contain state/city information
        script_tags = soup.find_all('script')
        for script in script_tags:
            if script.string:
                # Look for JSON data in scripts
                json_matches = re.findall(r'(\{[^{}]*"state"[^{}]*\})', script.string)
                for match in json_matches:
                    try:
                        data = json.loads(match)
                        if isinstance(data, dict) and 'state' in data:
                            states_cities.append(data)
                    except json.JSONDecodeError:
                        continue
        
        return states_cities
    
    def get_station_data_from_api(self) -> List[Dict]:
        """
        Attempt to get station data directly from API endpoints
        
        Returns:
            List of station data dictionaries
        """
        logging.info("Attempting to fetch data from API endpoints...")
        
        # Common API endpoints that might exist
        api_endpoints = [
            "aqi_dashboard_data.php",
            "get_aqi_data.php", 
            "station_data.json",
            "realtime_data.php",
            "dashboard_data.json"
        ]
        
        all_stations = []
        
        for endpoint in api_endpoints:
            url = urllib.parse.urljoin(self.api_base_url, endpoint)
            response = self.make_request(url)
            
            if response:
                try:
                    # Try to parse as JSON
                    data = response.json()
                    if isinstance(data, list):
                        all_stations.extend(data)
                    elif isinstance(data, dict) and 'stations' in data:
                        all_stations.extend(data['stations'])
                    elif isinstance(data, dict) and 'data' in data:
                        all_stations.extend(data['data'])
                except json.JSONDecodeError:
                    # If not JSON, try to parse HTML
                    soup = BeautifulSoup(response.content, 'html.parser')
                    stations = self.parse_station_data_from_html(soup)
                    all_stations.extend(stations)
        
        return all_stations
    
    def parse_station_data_from_html(self, soup: BeautifulSoup) -> List[Dict]:
        """
        Parse station data from HTML content
        
        Args:
            soup: BeautifulSoup object of the HTML content
            
        Returns:
            List of parsed station data
        """
        stations = []
        
        # Look for tables containing AQI data
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            headers = []
            
            # Extract headers
            header_row = rows[0] if rows else None
            if header_row:
                headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]
            
            # Extract data rows
            for row in rows[1:]:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= len(headers):
                    station_data = {}
                    for i, cell in enumerate(cells[:len(headers)]):
                        if i < len(headers):
                            station_data[headers[i]] = cell.get_text(strip=True)
                    
                    if station_data:
                        stations.append(station_data)
        
        # Look for div elements that might contain station data
        station_divs = soup.find_all('div', class_=re.compile(r'station|aqi|monitor', re.I))
        for div in station_divs:
            station_info = self.extract_station_info_from_div(div)
            if station_info:
                stations.append(station_info)
        
        return stations
    
    def extract_station_info_from_div(self, div) -> Optional[Dict]:
        """
        Extract station information from a div element
        
        Args:
            div: BeautifulSoup div element
            
        Returns:
            Dictionary with station information or None
        """
        station_data = {}
        
        # Extract text content
        text = div.get_text(strip=True)
        
        # Look for patterns in the text
        aqi_match = re.search(r'AQI[:\s]*(\d+)', text, re.I)
        if aqi_match:
            station_data['AQI'] = aqi_match.group(1)
        
        # Look for station name
        name_patterns = [
            r'Station[:\s]*([^,\n]+)',
            r'Location[:\s]*([^,\n]+)',
            r'^([^,\n]+)(?=,|\n|AQI)'
        ]
        
        for pattern in name_patterns:
            match = re.search(pattern, text, re.I)
            if match:
                station_data['Station'] = match.group(1).strip()
                break
        
        # Look for pollutant values
        for pollutant in self.pollutants:
            pattern = rf'{pollutant}[:\s]*(\d+\.?\d*)'
            match = re.search(pattern, text, re.I)
            if match:
                station_data[pollutant] = match.group(1)
        
        return station_data if station_data else None
    
    def scrape_main_dashboard(self) -> List[Dict]:
        """
        Scrape the main dashboard page for AQI data
        
        Returns:
            List of station data dictionaries
        """
        logging.info("Scraping main dashboard...")
        
        response = self.make_request(self.base_url)
        if not response:
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Parse station data from the HTML
        stations = self.parse_station_data_from_html(soup)
        
        # Look for embedded JavaScript data
        script_tags = soup.find_all('script')
        for script in script_tags:
            if script.string:
                # Look for JSON arrays or objects containing station data
                json_patterns = [
                    r'var\s+\w+\s*=\s*(\[.*?\]);',
                    r'data\s*:\s*(\[.*?\])',
                    r'stations\s*:\s*(\[.*?\])'
                ]
                
                for pattern in json_patterns:
                    matches = re.findall(pattern, script.string, re.DOTALL)
                    for match in matches:
                        try:
                            data = json.loads(match)
                            if isinstance(data, list):
                                stations.extend(data)
                        except json.JSONDecodeError:
                            continue
        
        return stations
    
    def process_station_data(self, raw_stations: List[Dict]) -> List[Dict]:
        """
        Process and normalize raw station data
        
        Args:
            raw_stations: List of raw station data dictionaries
            
        Returns:
            List of processed and normalized station data
        """
        processed_data = []
        current_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        for station in raw_stations:
            if not station:
                continue
                
            # Extract basic station information
            station_name = station.get('Station', station.get('station_name', 
                                     station.get('name', 'Unknown Station')))
            state = station.get('State', station.get('state', 
                               station.get('State_Name', 'Unknown State')))
            latitude = station.get('Latitude', station.get('lat', 
                                  station.get('latitude', None)))
            longitude = station.get('Longitude', station.get('lng', 
                                   station.get('longitude', None)))
            
            # Convert coordinates to float if they exist
            try:
                latitude = float(latitude) if latitude else None
                longitude = float(longitude) if longitude else None
            except (ValueError, TypeError):
                latitude = longitude = None
            
            # Extract pollutant data
            for pollutant in self.pollutants:
                value = None
                
                # Try different key variations
                possible_keys = [
                    pollutant,
                    pollutant.lower(),
                    pollutant.upper(),
                    pollutant.replace('.', ''),
                    f"{pollutant}_value",
                    f"{pollutant}_concentration"
                ]
                
                for key in possible_keys:
                    if key in station:
                        try:
                            value = float(station[key])
                            break
                        except (ValueError, TypeError):
                            continue
                
                # Only add if we have a valid value
                if value is not None:
                    processed_data.append({
                        'Station': station_name,
                        'State': state,
                        'Latitude': latitude,
                        'Longitude': longitude,
                        'Pollutant': pollutant,
                        'Value': value,
                        'Timestamp': current_timestamp
                    })
        
        return processed_data
    
    def scrape_all_data(self) -> pd.DataFrame:
        """
        Main method to scrape all available AQI data
        
        Returns:
            Pandas DataFrame with all collected data
        """
        logging.info("Starting comprehensive AQI data scraping...")
        
        all_stations = []
        
        # Method 1: Try API endpoints
        api_stations = self.get_station_data_from_api()
        all_stations.extend(api_stations)
        logging.info(f"Collected {len(api_stations)} stations from API endpoints")
        
        # Method 2: Scrape main dashboard
        dashboard_stations = self.scrape_main_dashboard()
        all_stations.extend(dashboard_stations)
        logging.info(f"Collected {len(dashboard_stations)} stations from dashboard")
        
        # Process the collected data
        processed_data = self.process_station_data(all_stations)
        logging.info(f"Processed {len(processed_data)} data points")
        
        # Create DataFrame
        if processed_data:
            df = pd.DataFrame(processed_data)
            
            # Remove duplicates
            df = df.drop_duplicates(subset=['Station', 'Pollutant'], keep='last')
            
            # Sort by station and pollutant
            df = df.sort_values(['Station', 'Pollutant'])
            
            logging.info(f"Final dataset contains {len(df)} unique data points")
            return df
        else:
            logging.warning("No data collected")
            return pd.DataFrame(columns=['Station', 'State', 'Latitude', 'Longitude', 
                                       'Pollutant', 'Value', 'Timestamp'])
    
    def save_to_csv(self, df: pd.DataFrame, filename: str = "cpcb_aqi_data.csv"):
        """
        Save DataFrame to CSV file
        
        Args:
            df: DataFrame to save
            filename: Output filename
        """
        try:
            df.to_csv(filename, index=False)
            logging.info(f"Data saved to {filename}")
            print(f"Successfully saved {len(df)} records to {filename}")
        except Exception as e:
            logging.error(f"Failed to save data to CSV: {str(e)}")
            raise

def main():
    """
    Main function to run the scraper
    """
    print("CPCB Air Quality Data Scraper")
    print("=" * 40)
    
    try:
        # Initialize scraper
        scraper = CPCBAirQualityScraper()
        
        # Scrape data
        print("Starting data collection...")
        df = scraper.scrape_all_data()
        
        if not df.empty:
            # Display summary
            print(f"\nData Collection Summary:")
            print(f"Total records: {len(df)}")
            print(f"Unique stations: {df['Station'].nunique()}")
            print(f"States covered: {df['State'].nunique()}")
            print(f"Pollutants monitored: {', '.join(df['Pollutant'].unique())}")
            
            # Show sample data
            print(f"\nSample data:")
            print(df.head(10).to_string())
            
            # Save to CSV
            scraper.save_to_csv(df)
            
            # Display data summary by pollutant
            print(f"\nData summary by pollutant:")
            pollutant_summary = df.groupby('Pollutant')['Value'].agg(['count', 'mean', 'min', 'max']).round(2)
            print(pollutant_summary.to_string())
            
        else:
            print("No data was collected. This might be due to:")
            print("1. Website structure changes")
            print("2. Network connectivity issues") 
            print("3. Rate limiting or blocking")
            print("4. Different API endpoints")
            print("\nPlease check the log file for more details.")
    
    except Exception as e:
        logging.error(f"Scraping failed: {str(e)}")
        print(f"Error occurred: {str(e)}")
        print("Check the log file for more details.")

if __name__ == "__main__":
    main()
